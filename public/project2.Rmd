---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Rachel David; rpd554; SDS348"
date: '2020-05-01'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)

class_diag<-function(probs,truth){
  
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  prediction<-ifelse(probs>.5,1,0)
  acc=mean(truth==prediction)
  sens=mean(prediction[truth==1]==1)
  spec=mean(prediction[truth==0]==0)
  ppv=mean(truth[prediction==1]==1)
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}
```

# Modeling



- **0.** 
```{r}
#INTRODUCING DATA 
library(tidyr)
pop <-as.data.frame(population)

library(VGAM)
olympics <- as.data.frame(olym12)
pop <- pop %>% pivot_wider(names_from ="year", values_from="population")

pop1 <- pop[-c(2:18, 20)]
library(dplyr)
as.data.frame(apply(olympics,2,function(x)gsub('\\s+', '',x)))
pop1 <- as.data.frame(apply(pop1,2,function(x)gsub('\\s+', '',x)))
names(pop1)[names(pop1) == "2012"] <- "popul"
pop1$popul <- as.numeric(as.character(pop1$popul))
pop1 %>% mutate_if(is.factor, as.character) -> pop1

test <- pop1
pop1$country <- gsub("China,HongKongSAR", "HongKong", pop1$country)
pop1$country <- gsub("UnitedStatesofAmerica", "UnitedStates", pop1$country)
pop1$country <- gsub("UnitedKingdomofGreatBritainandNorthernIreland", "GreatBritain", pop1$country)
pop1$country <- gsub("RussianFederation", "Russia", pop1$country)
pop1$country <- gsub("RepublicofKorea", "SouthKorea", pop1$country)
pop1$country <- gsub("DemocraticPeople'sSouthKorea", "NorthKorea", pop1$country)
pop1$country <- gsub("Iran\\(IslamicRepublicof\\)", "Iran", pop1$country)
pop1$country <- gsub("Venezuela\\(BolivarianRepublicof\\)", "Venezuela", pop1$country)
pop1$country <- gsub("VietNam", "Vietnam", pop1$country)
outlier <- olympics %>% anti_join(pop1,by=c("country"="country"))%>% distinct(country)
outlier2 <- pop1 %>% anti_join(olympics,by=c("country"="country"))%>% distinct(country)
pop1$country <- gsub("RepublicofMoldova", "Moldova", pop1$country)

proj2 <- inner_join(pop1, olympics, by=c("country"="country"))
proj2 <-proj2 %>% mutate (size = case_when(popul<10000000 ~ "small", 
popul<=50000000 & popul>=10000000 ~ "medium", popul>50000000 ~ "big"))
proj2<-proj2%>%mutate(goldmedalwinner=ifelse(gold>0,1,0))
proj2<-proj2%>%mutate(top2=ifelse(gold>0|silver>0,1,0))

proj2$top2 <- as.factor(proj2$top2)
proj2<-proj2%>%mutate(y=ifelse(top2==1,1,0))




```
The dataset I am using is a 2012 Olympics dataset, which contains the variables: country,  population, overall ranking (in terms of total number of medals), # of gold, silver, and bronze medals, totalmedals (sum of all medals won by the country), size (which categorizes by big, medium, and small countries by population size), goldmedalwinner (which is a binary variable indicating whether the country did or did not win a gold medal), and top2 (which is a binary variable indicating if the country has won at least 1 gold or silver medal). Each variable has 84 observations. 

- **1.** 
```{r}
#MANOVA testing 
man1<-manova(cbind(ranking,gold,silver,bronze)~size, data=proj2) 
summary(man1)

#univariate anovas 
summary.aov(man1) 

#post-hoc t tests 
pairwise.t.test(proj2$ranking,proj2$size, p.adj="none")
pairwise.t.test(proj2$gold,proj2$size, p.adj="none")
pairwise.t.test(proj2$silver,proj2$size, p.adj="none")
pairwise.t.test(proj2$bronze,proj2$size, p.adj="none")

#adjusted bonferonni correction 
0.05/17

ggplot(proj2, aes(x = ranking, y = gold)) +  geom_point(alpha = .5) + 
  geom_density_2d(h=2) + coord_fixed() + facet_wrap(~size)

ggplot(proj2, aes(x = gold, y = silver)) +  geom_point(alpha = .5) + 
  geom_density_2d(h=2) + coord_fixed() + facet_wrap(~size)

ggplot(proj2, aes(x = silver, y = bronze)) +  geom_point(alpha = .5) + 
  geom_density_2d(h=2) + coord_fixed() + facet_wrap(~size)

covmats<-proj2%>%group_by(size)%>%do(covs=cov(.[3:6])) 
for(i in 1:3){print(as.character(covmats$size[i])); print(covmats$covs[i])}


```
     After running the manova test, we had a significant p-value of less than 0.05, and so we can reject 
the null hypothesis and conclude that there was a significant mean difference across levels of one my 
categorical variables (size). 

     We then performed follow-up one-way, univariate ANOVAs for each variable and found that for 
all 4 of these numeric variables, they were significant, which means that for ranking, gold, 
silver, and bronze, at least one of the size classifications have mean differences that differs 
significantly. 
 
     We then performed individual post-hoc t-tests and found that the big and medium groups differ 
and the big and small groups differ for ranking, gold,  silver, and bronze, as these groups had 
p-values less than the alpha of 0.05, so we can reject the null hypothesis that there is no difference 
between these groups (when using the unadjusted probability of a type-1 error, which is 0.05). 

     We conducted a total of 1 MANOVA, 4 ANOVAs, and 12 t-tests for a total of 17 tests. With 
the bonferroni correction, the alpha value we should be using is 0.00294. With this adjusted 
correction, there is no longer a significant difference between big and medium size for ranking, 
but all other big and medium and big and small differences are significant for gold, silver, 
and bronze as well as big and small for ranking.

     When considering the assumptions for a MANOVA, we see that 
we have independent observations, but these are not random samples as the data is from certain 
countries. When examining bivariate density plots for each group revealed stark deparures from 
multivariate normality, so this assumption is likely not met. Examination of covariance matrices 
with covmats for each group did not show relative homogeneity of within-group covariance matrices, 
so this assumption is likely not met. We do see linear relationships between our dependent variables, 
so this assumption is likely met. We do see a few extreme univariate or multivariate outliers, 
so this assumptions is likely not met.  Finally, we see no multicolinearity, as the dependent 
variables are not too correlated, so this assumption is likely met. 


- **2.** 
```{r}
#Randomization Test 
rand <- proj2%>%dplyr::filter(!size=="medium")
  
rand%>%group_by(size)%>%
  summarize(means=mean(totalmedal))%>%summarize(`mean_diff:`=diff(means))%>% pull #26.150

#loop
diffs<-vector()

for(i in 1:5000){
  temp <- rand %>% mutate(totalmedal=sample(rand$totalmedal))
  diffs[i] <- temp %>% summarize(mean(totalmedal[size=="big"])-mean(totalmedal[size=="small"])) %>% pull
}

mean(diffs>26.150 | diffs< -26.150)

#plot visualizing null distribution and test statistic 
{hist(diffs,main="",ylab=""); abline(v = -26.150,col="red")}

```
  
  The null hypothesis is that there is no difference between average means of total medals received for big and small size countries. The alternate hypothesis is that there is a difference between average means of total medals received for big and small size countries.  
  After running the randomization test, we see a result of a two-tailed p-value of 0, which means that there are 0 mean differences that are more extreme than the actual mean difference of +/-26.150. This shows us that we can reject the null hypothesis, as we have a p-value less than 0.05, and if the null hypothesis were true, it would be very rare to get the mean difference that we saw. 
  
  When graphing this null distribution and test statistic, we cannot even see the red line indicating the true mean difference of -26.150 as the graph's x-axis stops at -10, which confirms the unlikelihood of actually observing this mean difference if the null hypothesis were true. 

- **3.** 

```{r}
#LINEAR REGRESSION 

#mean-centering numeric variables 
proj2$popul_c <- proj2$popul - mean(proj2$popul) 
proj2$rank_c <- proj2$ranking - mean(proj2$ranking)
fit<-lm(gold~popul_c*rank_c, data= proj2) 
#interpret the coefficient estimates 
summary(fit) #rank and interactions are significant 

###plot the regression 
fit<-lm(gold ~ popul_c * rank_c, data=proj2)
new1<-proj2
new1$popul_c<-mean(proj2$popul_c)
new1$mean<-predict(fit,new1)
new1$popul_c<-mean(proj2$popul_c)+sd(proj2$popul_c)
new1$plus.sd<-predict(fit,new1)
new1$popul_c<-mean(proj2$popul_c)-sd(proj2$popul_c)
new1$minus.sd<-predict(fit,new1)

mycols<-c("#619CFF","#F8766D","#00BA38")
names(mycols)<-c("-1 sd","mean","+1 sd")
mycols=as.factor(mycols)

ggplot(proj2,aes(rank_c,gold),group=mycols)+geom_point()+geom_line(data=new1,aes(y=mean,color="mean"))+geom_line(data=new1,aes(y=plus.sd,color="+1 sd"))+geom_line(data=new1,aes(y=minus.sd,color="-1 sd"))+scale_color_manual(values=mycols)+labs(color="Population")+theme(legend.position=c(.9,.6))+ggtitle("Linear regression predicting gold medals from country population and ranking")+xlab("Ranking (mean-centered)")+ylab("Gold medals")
###

#check homoskedasticity, linearity, and normality 
library(lmtest)
library(sandwich)
bptest(fit)
resids<-fit$residuals
fitvals<-fit$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, col="red")
ggplot()+geom_histogram(aes(resids),bins=20)
ggplot()+geom_qq(aes(sample=resids))+geom_qq_line(aes(sample=resids),color="red")

#recompute with robust standard errors 
coeftest(fit, vcov = vcovHC(fit)) #only rank is significant 

#proportion of variation explained by model 
summary(fit)$r.sq

```
  The intercept coefficient shows us that with an avergage population and average rank,we expect an average country in this dataset to have 3.1259 medals; for every 1 person increase in population, we expect number of gold medals to increase by almost 0, 0.00000000561; for every 1 rank increase (which means the country was ranked poorer overall), we expect number of gold medals to decrease by 0.1746. The interaction of average population and average rank is not significant and has a coefficient estimate of essentially 0, so this interaction appears to have almost no effect on number of gold medals won. As the coefficient estimate is negative, it means that higher values for popul_c lead to a weaker/less steep slope for rank_c on the response. Furthermore, for every one unit increase in popul_c, the slope for rating gets 5.312E-10 smaller. 
  
  We then ran a bptest() and as the p-value was 0.01962, we can reject the null hypothesis as it is smaller than the alpha value of 0.05 and conclude that this data does not show homoskedasticity. After graphing to check for linearity and normality assumptions, we can conclude that our data is overall fairly linear with some large outliers and we can conclude that our data does not show normality as we see large outliers. 
 
  After recomputing regression results with robust standard errors we see that rank_centered is significant as it has a p-value less than the alpha value of 0.05, so we can reject the null hypothesis and conclude that rank does have a significant effect on number of gold medals won. In the robust standard errors, we see only rank as significant, while the original model (without robust standrad errors) indicated that both rank and the interaction of rank:population was significant. We see larger p-values overall for the robust standard errors. 
 
  The proportion of variation in the outcome that this model explains is 0.6423. This tells us that 64.23% of the variation in number of gold medals can be explained by rank, population, and the interaction of rank and population. 

- **4.** 

```{r}
#REGRESSION MODEL WITH BOOTSTRAPPED SE

#residuals bootstrapped standard errors 
set.seed(348)
fit2<-lm(gold~popul_c*rank_c, data= proj2) 
resids<-fit2$residuals
fitted<-fit2$fitted.values 

resid_resamp<-replicate(5000,{    
  new_resids<-sample(resids,replace=TRUE)
  proj2$new_gold<-fitted+new_resids
  fit2<-lm(new_gold~popul_c*rank_c,data=proj2) 
  coef(fit2) 
})

resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)

#model with bootstrapped standard errors
samp_distn<-replicate(5000, {
  boot_dat<-sample_frac(proj2, replace=T) 
  fit2<-lm(gold~popul_c*rank_c,data=boot_dat)
  coef(fit2)
})

samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)

```
  The original SE we saw were 0.5167 for intercept, 2.96E-9 for population_c, 0.0219 for rank_c, and 9.633E-12 for popul_c:rank_c. The p-values for the original model were significant for rank_c and popul_c:rank_c and intercept: 4.399E-8 for intercept, 0.0219 for rank_c, and 4.166E-7 for popul_c:rank_c.
  
  With robust standard errors, we saw larger standard errors overall: 0.5697 for intercept, 2.253E-8 for population_c, 0.040 for rank_c, and 1.61E-9 for popul_c:rank_c. We see from the p-values that only the intercept and rank_c are significant (p-values less than 0.05): 4.65E-7 for intercept, 0.8041 for population_c, 0.0000383 for rank_c, and 0.7423 for popul_c:rank_c.
  
  The standard errors we saw for the residuals when doing bootstrapped standard errors were comparable to what we saw in the original data (not with robust standard errors):
0.500 for intercept, 2.826E-9 for population_c, 0.0213 for rank_c, and 9.394E-11 for popul_c:rank_c.
  
  The standard errors we saw for the sample data we calculated when doing bootstrapped standard errors were overall higher than any of the other models, but comparable: 0.8337 for intercept, 1.838E-8 for population_c, 0.041 for rank_c, and 8.820E-10 for popul_c:rank_c. As these standard errors were comparable to those we saw earlier when using the robust standard errors, we can conlude that the same variables are likely significant with a p-value less than 0.05, so this would be the rank_c variable and popul_c:rank_c. 

- **5.** 
    
```{r}
#LOGISTIC REGRESSION

#linear regression on binary categorical variable 
fit3<-glm(top2~size+rank_c, family="binomial", data=proj2) 
coeftest(fit3)
#interpret coefficient estimates
exp(coef(fit3))

#confusion matrix 
prob<-predict(fit3,type="response")
table(predict=as.numeric(prob>.5),truth=proj2$top2)%>%addmargins

(11+73)/84 #accuracy
73/73 #tpr/sensitivity
11/11 #true negative rate/specificity
73/73 #ppv (precision)

#plot density of log-odds 
loggraph <- proj2
loggraph$logit<-predict(fit3,type="link")

loggraph%>%ggplot()+geom_density(aes(logit,color=top2,fill=top2), alpha=.5)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("logit (log-odds)")+
  geom_rug(aes(logit,color=top2)) +ggtitle("Density of log-odds by Gold or Silver Winner")

#generate ROC curve and calculate AUC
library(plotROC)
library(pROC)
ROCplot<-ggplot(proj2)+geom_roc(aes(d=y,m=prob), n.cuts=0)+ 
  geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2)
ROCplot

calc_auc(ROCplot)

#10-fold CV 
set.seed(1234)
k=10

data<-proj2[sample(nrow(proj2)),]
folds<-cut(seq(1:nrow(proj2)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  truth<-test$top2
  
  fit1e<- glm(top2~size+rank_c,data=train, family="binomial")
  probs<-predict(fit1e,newdata = test,type="response")
 
   diags<-rbind(diags,class_diag(probs,truth))
  
}

summarize_all(diags,mean)


```
  From the coefficients of the logistic regression, we see that controlling for size of the country, for every 1 position increase in rank (meaning the country is lower in total medals received), the odds of them receiving a silver or gold medal decreased by a factor of 0.001 (not-significant). We see that controlling for ranking, there is not a significant difference in receiving gold or silver medals between small, medium, and large countries. Controlling for ranking, odds of receiving a gold or silver medal for a small country is 1.719E19 times that of a large country, and odds of receiving a gold or silver medal for a medium country is 2.143E17 that of a large country. However for a large country, when rank is average, odds of receiving a gold or silver medal is 1.748E73.  
  
  I created a confusion matrix and see that there was an accuracy of 1, sensitivity (or true positive rate) of 1, specificity (or true negative rate) of 1, and a ppv (precision) of 1. This shows that our model correctly identified all of the cases. 
 
  After graphing the ggplot with the density of log-odds, we see when logit>0, we predicted true that the country was in the top2, so all the blue to the right of the black line shows our true positive. When logit<0, we predict it is false that the country was in the top2, so all the red to the right of the black line shows our true negative rate. 
 
  After graphing the ROC plot, we see an AUC value of 1 -- this shows us that our model is perfectly predicting the outcome and that none of the predictions would be off. 
 
  After doing the 10-fold CV, we see out-of-sample Accuracy of 0.9875, Sensitivity of 0.9833, Recall of 1, and AUC of 1. We see that our 10-fold diagnositcs were slightly lower than those computed with all of our data because we are using random sampling of our data, and our AUC is still 1, which shows that our model is still predicting perfectly. 

- **6.** 

```{r}
#LASSO REGRESSION

#lasso regression
lassodata<-proj2%>%dplyr::select(-country, -y)
library(glmnet)
y<-as.matrix(lassodata$top2) 
x<-model.matrix(top2~.,data=lassodata) [,-1]
x<-scale(x)
cv<-cv.glmnet(x,y, family="binomial")
lasso<-glmnet(x,y,family="binomial",lambda=cv$lambda.1se)
coef(lasso)

proj2$bronze_c <- proj2$bronze - mean(proj2$bronze) 

#10-fold cv with lasso selected variables 
set.seed(1234)
k=10
data<-proj2[sample(nrow(proj2)),]
folds<-cut(seq(1:nrow(proj2)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$top2
  
  fit1e<- glm(top2~rank_c+bronze_c,data=train, family="binomial")
  probs<-predict(fit1e,newdata = test,type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean)


```
  The variables with non-zero values after the lasso regression was ranking and bronze.   
 
  After performing the 10-fold CV using the variables identified by lasso (ranking and bronze, both were mean centered), we see that the lasso variables model's out-of-sample accuracy was 0.9889, which is higher than the accuracy of the out-of-sample model we used for 10-fold CV, which was 0.9875. The sensitivity for this lasso-selected variable model was 0.9875, the ppv was 1, and the auc was 1, again showing that our model would perfectly predict the data. 


...





